import json
import logging
from datetime import datetime, timedelta
from typing import Dict, Any, List
from zoneinfo import ZoneInfo

from openai import AsyncOpenAI
import os
import httpx

from config.settings import settings

logger = logging.getLogger(__name__)

class OpenAIService:
    def __init__(self):
        self.client = AsyncOpenAI(api_key=settings.OPENAI_API_KEY)
        self.model = settings.OPENAI_MODEL
    
    async def request_chat_completion(self, messages: List[Dict[str, str]], temperature: float = 0.7, max_tokens: int = 200) -> str:
        """Llama ë˜ëŠ” OpenAI ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì±„íŒ… ì‘ë‹µ ìƒì„± (í†µí•© ë©”ì„œë“œ)"""
        # Llama API ìš°ì„  ì‚¬ìš©
        if settings.LLM_API_URL or os.getenv("LLM_API_URL"):
            return await self._call_custom_model(messages, temperature, max_tokens)
        
        # OpenAI í´ë°±
        response = await self.client.chat.completions.create(
            model=self.model,
            messages=messages,
            max_tokens=max_tokens,
            temperature=temperature
        )
        return response.choices[0].message.content.strip()

    async def _call_custom_model(self, messages: List[Dict[str, str]], temperature: float = 0.7, max_tokens: int = 500) -> str:
        """ì»¤ìŠ¤í…€ LLM (Llama ë“±) í˜¸ì¶œ - ìƒˆ API ìŠ¤í™"""
        url = settings.LLM_API_URL or os.getenv("LLM_API_URL")
        if not url:
            raise ValueError("LLM_API_URL not set")

        # ìƒˆ API ìŠ¤í™: messages ë°°ì—´ ê·¸ëŒ€ë¡œ ì „ì†¡
        payload = {
            "messages": messages,
            "temperature": temperature,
            "max_tokens": max_tokens
        }
        
        logger.info(f"[Llama API] ìš”ì²­ ì „ì†¡: {url}")
        logger.debug(f"[Llama API] Payload: {len(messages)}ê°œ ë©”ì‹œì§€")
        
        async with httpx.AsyncClient(timeout=120.0) as client:
            resp = await client.post(url, json=payload)
            resp.raise_for_status()
            data = resp.json()
            response_text = data.get("response", "")
            logger.info(f"[Llama API] ì‘ë‹µ ìˆ˜ì‹ : {len(response_text)}ì")
            return response_text

    def _get_current_time_info(self) -> str:
        """í˜„ì¬ ì‹œê°„ ì •ë³´ë¥¼ ë¬¸ìì—´ë¡œ ë°˜í™˜"""
        KST = ZoneInfo("Asia/Seoul")
        now = datetime.now(KST)
        
        # ìš”ì¼ì„ í•œê¸€ë¡œ ë³€í™˜
        weekday_map = {
            0: "ì›”ìš”ì¼",
            1: "í™”ìš”ì¼", 
            2: "ìˆ˜ìš”ì¼",
            3: "ëª©ìš”ì¼",
            4: "ê¸ˆìš”ì¼",
            5: "í† ìš”ì¼",
            6: "ì¼ìš”ì¼"
        }
        
        weekday_kr = weekday_map[now.weekday()]
        return now.strftime(f"%Yë…„ %mì›” %dì¼ {weekday_kr} %Hì‹œ %Më¶„ (í•œêµ­ ì‹œê°„)")
    
    async def generate_response(self, user_message: str, conversation_history: List[Dict[str, str]] = None) -> Dict[str, Any]:
        """ChatGPT APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ìƒì„±"""
        try:
            current_time = self._get_current_time_info()
            
            system_prompt = f"""ë‹¹ì‹ ì€ AI Joy Assistantì˜ ì¼ì • ì¡°ìœ¨ ë„ìš°ë¯¸ì…ë‹ˆë‹¤. 
ì‚¬ìš©ìì™€ ì¹œêµ¬ë“¤ì˜ ì¼ì •ì„ ì¡°ìœ¨í•˜ê³  ì•½ì†ì„ ì¡ëŠ” ê²ƒì„ ë„ì™€ì£¼ì„¸ìš”.

í˜„ì¬ ì‹œê°„: {current_time}

ì£¼ìš” ê¸°ëŠ¥:
1. ì¹œêµ¬ì™€ì˜ ì¼ì • ì¡°ìœ¨
2. ì•½ì† ì‹œê°„ ë° ì¥ì†Œ ì œì•ˆ
3. ì¼ì • ì¶©ëŒ í™•ì¸
4. ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ëŒ€í™”

## âš ï¸ ì ˆëŒ€ ê·œì¹™: ì •ë³´ë¥¼ ë§Œë“¤ì–´ë‚´ì§€ ë§ˆì„¸ìš”!

**ì‚¬ìš©ìê°€ ë§í•˜ì§€ ì•Šì€ ì •ë³´ëŠ” ì ˆëŒ€ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”!**
- ì‚¬ìš©ìê°€ ì¥ì†Œë¥¼ ë§í•˜ì§€ ì•Šì•˜ìœ¼ë©´ ì¥ì†Œë¥¼ ì¶”ì¸¡í•˜ê±°ë‚˜ ë§Œë“¤ì–´ë‚´ì§€ ë§ˆì„¸ìš”
- "ê°•ë‚¨", "í™ëŒ€" ë“± êµ¬ì²´ì ì¸ ì¥ì†Œëª…ì„ ì‚¬ìš©ìê°€ ë§í•˜ì§€ ì•Šì•˜ìœ¼ë©´ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”
- ì‚¬ìš©ìê°€ ë§í•œ ë‚´ìš©ë§Œ ì •í™•í•˜ê²Œ ì‚¬ìš©í•˜ì„¸ìš”

## âš ï¸ ì§§ì€ ì‘ë‹µ í•´ì„ ê·œì¹™

ì‚¬ìš©ìê°€ ì§§ê²Œ ë‹µí•˜ë©´ ëŒ€í™” ë§¥ë½ì„ ë³´ê³  í•´ì„í•˜ì„¸ìš”:
- "ì•„ë‹", "ì•„ë‹ˆ", "ëª°ë¼", "ë¯¸ì •" = ëë‚˜ëŠ” ì‹œê°„ì´ ì •í•´ì§€ì§€ ì•ŠìŒ â†’ ì‹œì‘ ì‹œê°„ë§Œìœ¼ë¡œ ë“±ë¡
- "ì‘", "ë„¤", "ê·¸ë˜" = í™•ì¸/ë™ì˜
- **"ì•„ë‹"ì„ "ì•ˆë…•"ìœ¼ë¡œ í•´ì„í•˜ì§€ ë§ˆì„¸ìš”!** ì´ê²ƒì€ ì¸ì‚¬ê°€ ì•„ë‹ˆë¼ "ì•„ë‹ˆ"ì˜ ì¤„ì„ë§ì…ë‹ˆë‹¤.

## âš ï¸ ê°€ì¥ ì¤‘ìš”í•œ ê·œì¹™: ëŒ€í™” ë§¥ë½ ê¸°ì–µ

**ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ë°˜ë“œì‹œ ê¸°ì–µí•˜ì„¸ìš”!** ì‚¬ìš©ìê°€ ì´ë¯¸ ë§í•œ ì •ë³´ë¥¼ ì ˆëŒ€ ë‹¤ì‹œ ë¬¼ì–´ë³´ì§€ ë§ˆì„¸ìš”.

ì¼ì • ë“±ë¡ ëŒ€í™” ì¤‘ì— ì§§ì€ ì‘ë‹µì´ ì˜¤ë©´, ê·¸ê²ƒì€ ëŒ€í™”ì˜ ì—°ì†ì…ë‹ˆë‹¤. ìƒˆë¡œìš´ ëŒ€í™”ê°€ ì•„ë‹™ë‹ˆë‹¤!

ì˜ˆì‹œ:
- ì‚¬ìš©ìê°€ "ì„œì ì— ê°€ì•¼ë¼"ë¼ê³  í–ˆìœ¼ë©´, ì¼ì • ë‚´ìš©ì€ ì´ë¯¸ "ì„œì  ë°©ë¬¸"ì…ë‹ˆë‹¤.
- ì‚¬ìš©ìê°€ "2ì‹œì—"ë¼ê³  í–ˆìœ¼ë©´, ì´ì „ ëŒ€í™”ì˜ ì¼ì • ì‹œê°„ì…ë‹ˆë‹¤.
- ì‚¬ìš©ìê°€ "ì•„ë‹"ì´ë¼ê³  í–ˆìœ¼ë©´, ëë‚˜ëŠ” ì‹œê°„ì´ ì—†ë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤. ë°”ë¡œ ë“±ë¡í•˜ì„¸ìš”!

## ì¼ì • ë“±ë¡ ìš”ì²­ ì‹œ:

### í•„ìˆ˜ ì •ë³´ í™•ì¸ ìˆœì„œ:
1. **ì¼ì • ë‚´ìš©** - ì´ë¯¸ ëŒ€í™”ì—ì„œ ì–¸ê¸‰ë˜ì—ˆëŠ”ì§€ í™•ì¸!
2. **ë‚ ì§œ** - ì´ë¯¸ ì–¸ê¸‰ë˜ì—ˆëŠ”ì§€ í™•ì¸
3. **ì‹œê°„** - ì—†ìœ¼ë©´ "ëª‡ ì‹œì— ê°€ì‹¤ ì˜ˆì •ì¸ê°€ìš”?"

### ì‹œê°„ ì •ë³´ í™•ì¸ (ë§¤ìš° ì¤‘ìš”):
- **ì‹œê°„ ì •ë³´ê°€ ì•„ì˜ˆ ì—†ìœ¼ë©´**: "ëª‡ ì‹œì— ê°€ì‹¤ ì˜ˆì •ì¸ê°€ìš”?"
- **ì‹œì‘ ì‹œê°„ë§Œ ë§í–ˆì„ ë•Œ**: ì ˆëŒ€ "ë“±ë¡í–ˆìŠµë‹ˆë‹¤"ë¼ê³  í•˜ì§€ ë§ê³ , **"ëë‚˜ëŠ” ì‹œê°„ë„ ì •í•´ì¡Œë‚˜ìš”?"** ë¼ê³  ë¨¼ì € ë¬¼ì–´ë³´ì„¸ìš”.
- **"ì•„ë‹", "ì•„ë‹ˆ", "ëª°ë¼", "ë¯¸ì •"ì´ë©´**: ê·¸ë•Œ ë¹„ë¡œì†Œ "ë„¤, ì˜¤í›„ 00ì‹œë¡œ ë“±ë¡í–ˆìŠµë‹ˆë‹¤!"ë¼ê³  ì™„ë£Œ ë©”ì‹œì§€ë¥¼ ë³´ë‚´ì„¸ìš”.

### ì˜¬ë°”ë¥¸ ëŒ€í™” ì˜ˆì‹œ:
```
ì‚¬ìš©ì: "ë‚´ì¼ ë™ìƒ ë°ë¦¬ê¸° ì¼ì • ë“±ë¡í•´ì¤˜"
AI: "ë„¤, ë‚´ì¼ ë™ìƒ ë°ë¦¬ê¸° ì¼ì •ì„ ë“±ë¡í•´ë“œë¦´ê²Œìš”. ëª‡ ì‹œì— ê°€ì‹¤ ì˜ˆì •ì¸ê°€ìš”?"

ì‚¬ìš©ì: "2ì‹œì— ê°ˆê±°ì•¼"
AI: "ì•Œê² ìŠµë‹ˆë‹¤! ë‚´ì¼ ì˜¤í›„ 2ì‹œì— 'ë™ìƒ ë°ë¦¬ê¸°' ì¼ì •ì„ ë“±ë¡í• ê²Œìš”. ëë‚˜ëŠ” ì‹œê°„ë„ ì •í•´ì¡Œë‚˜ìš”?"

ì‚¬ìš©ì: "ì•„ë‹"
AI: "ë„¤, ë‚´ì¼ ì˜¤í›„ 2ì‹œ 'ë™ìƒ ë°ë¦¬ê¸°' ì¼ì •ìœ¼ë¡œ ë“±ë¡í–ˆìŠµë‹ˆë‹¤! âœ…"
```

ì‹œê°„ ê´€ë ¨ ì§ˆë¬¸ì— ë‹µí•  ë•ŒëŠ” í˜„ì¬ ì‹œê°„ì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
ìš”ì¼ ê³„ì‚°ì´ í•„ìš”í•œ ê²½ìš° í˜„ì¬ ìš”ì¼ì„ ê¸°ì¤€ìœ¼ë¡œ ì •í™•íˆ ê³„ì‚°í•˜ì„¸ìš”.
í•­ìƒ ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” í†¤ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”."""

            messages = [{"role": "system", "content": system_prompt}]
            
            if conversation_history:
                # ìµœê·¼ 10ê°œ ëŒ€í™”ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš© (TPM ì œí•œ ê³ ë ¤í•˜ì—¬ ì¶•ì†Œ)
                recent_history = conversation_history[-10:]
                logger.info(f"[OpenAI] ëŒ€í™” íˆìŠ¤í† ë¦¬ {len(recent_history)}ê°œ ì‚¬ìš©")
                for msg in recent_history:
                    if msg.get("type") == "user":
                        messages.append({"role": "user", "content": msg["message"]})
                        logger.debug(f"[OpenAI] íˆìŠ¤í† ë¦¬ - User: {msg['message'][:50]}...")
                    elif msg.get("type") == "assistant":
                        messages.append({"role": "assistant", "content": msg["message"]})
                        logger.debug(f"[OpenAI] íˆìŠ¤í† ë¦¬ - AI: {msg['message'][:50]}...")
            
            messages.append({"role": "user", "content": user_message})
            logger.info(f"[OpenAI] í˜„ì¬ ë©”ì‹œì§€: {user_message}")
            
            # Llama API ìš°ì„  ì‚¬ìš©
            if settings.LLM_API_URL or os.getenv("LLM_API_URL"):
                ai_response = await self._call_custom_model(messages, temperature=0.7, max_tokens=500)
                logger.info(f"[Llama API] ì‘ë‹µ ìƒì„± ì™„ë£Œ: {len(ai_response)}ì")
                return {
                    "status": "success",
                    "message": ai_response,
                    "usage": {}
                }

            response = await self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                max_tokens=500,
                temperature=0.7
            )
            
            ai_response = response.choices[0].message.content
            
            logger.info(f"OpenAI API ì‘ë‹µ ìƒì„± ì™„ë£Œ: {len(ai_response)}ì")
            
            return {
                "status": "success",
                "message": ai_response,
                "usage": {
                    "prompt_tokens": response.usage.prompt_tokens,
                    "completion_tokens": response.usage.completion_tokens,
                    "total_tokens": response.usage.total_tokens
                }
            }
            
        except Exception as e:
            error_msg = str(e)
            logger.error(f"OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {error_msg}")
            
            user_msg = "ì£„ì†¡í•´ìš”, ì§€ê¸ˆ ì ì‹œ ìƒê°ì´ ì•ˆ ë‚˜ë„¤ìš”. ğŸ¤¯ ì ì‹œ í›„ ë‹¤ì‹œ ë§í•´ì£¼ì‹œê² ì–´ìš”?"
            if "insufficient_quota" in error_msg:
                user_msg = "API ì‚¬ìš©ëŸ‰ í•œë„ê°€ ì´ˆê³¼ë˜ì—ˆì–´ìš”. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•´ì£¼ì„¸ìš”. ğŸ¥²"
            elif "rate limit" in error_msg.lower():
                user_msg = "ì§€ê¸ˆ ë„ˆë¬´ ë§ì€ ëŒ€í™”ê°€ ì˜¤ê³  ê°€ê³  ìˆì–´ìš”. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”! ğŸ•’"
                
            return {
                "status": "error",
                "message": user_msg,
                "error": error_msg
            }
    
    async def extract_schedule_info(self, message: str) -> Dict[str, Any]:
        """ë©”ì‹œì§€ì—ì„œ ì¼ì • ê´€ë ¨ ì •ë³´ ì¶”ì¶œ"""
        try:
            current_time = self._get_current_time_info()
            
            # í˜„ì¬ ì‹œê°„ ìƒì„¸ ì •ë³´ (YYYY-MM-DD í˜•ì‹ í¬í•¨)
            now_dt = datetime.now(ZoneInfo("Asia/Seoul"))
            today_str = now_dt.strftime("%Y-%m-%d")
            
            system_prompt = f"""ë‹¤ìŒ ë©”ì‹œì§€ì—ì„œ ì¼ì • ê´€ë ¨ ì •ë³´ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.
í˜„ì¬ ì‹œê°: {current_time}
ì˜¤ëŠ˜ ë‚ ì§œ(ê¸°ì¤€): {today_str}

**ì¤‘ìš”: ë°˜ë“œì‹œ ìœ íš¨í•œ JSONë§Œ ë°˜í™˜í•˜ì„¸ìš”.**

JSON ë°˜í™˜ í˜•ì‹:
{{
    "friend_name": "ì¹œêµ¬ ì´ë¦„",
    "friend_names": ["ì¹œêµ¬1", "ì¹œêµ¬2"],
    "date": "í…ìŠ¤íŠ¸ ë‚ ì§œ (ì˜ˆ: ì´ë²ˆì£¼ ê¸ˆìš”ì¼)",
    "start_date": "YYYY-MM-DD (ë²”ìœ„ ì‹œì‘)",
    "end_date": "YYYY-MM-DD (ë²”ìœ„ ì¢…ë£Œ)",
    "time": "ì‹œê°„ í…ìŠ¤íŠ¸ (ì˜ˆ: ì €ë…)",
    "start_time": "HH:MM (24ì‹œê°„ì œ)",
    "end_time": "HH:MM (24ì‹œê°„ì œ)",
    "activity": "í™œë™ ë‚´ìš©",
    "title": "ì¼ì • ì œëª©",
    "location": "ì¥ì†Œ",
    "has_schedule_request": true/false,
    "missing_fields": ["date", "time", "location"] (ëˆ„ë½ëœ í•„ìˆ˜ ì •ë³´ ë¦¬ìŠ¤íŠ¸)
}}

## 1. ë‚ ì§œ ë²”ìœ„ ë³€í™˜ ê·œì¹™ (ì˜¤ëŠ˜: {today_str} ê¸°ì¤€)
- "ì´ë²ˆ ë‹¬": ì˜¤ëŠ˜ë¶€í„° ì´ë²ˆ ë‹¬ ë§ì¼ê¹Œì§€ (start_date ~ end_date)
- "ë‹¤ìŒ ì£¼": ë‹¤ìŒ ì£¼ ì›”ìš”ì¼ ~ ì¼ìš”ì¼
- "ì£¼ë§": ì´ë²ˆ ì£¼ í† ìš”ì¼ ~ ì¼ìš”ì¼ (ì´ë¯¸ ì§€ë‚¬ìœ¼ë©´ ë‹¤ìŒ ì£¼ ì£¼ë§)
- "í‰ì¼": ì›”~ê¸ˆ
- "ì˜¤ëŠ˜": ì˜¤ëŠ˜ ë‚ ì§œ
- "ë‚´ì¼": ì˜¤ëŠ˜ + 1ì¼

## 2. ì‹œê°„ ë³€í™˜ ê·œì¹™ (ë§¤ìš° ì¤‘ìš”!)
- "ì•„ì¹¨": start_time="09:00", end_time="11:00"
- "ì ì‹¬": start_time="12:00", end_time="14:00"
- "ì €ë…": start_time="18:00", end_time="22:00"
- **"ì˜¤í›„" + ìˆ«ì**: ë°˜ë“œì‹œ 12ë¥¼ ë”í•˜ì„¸ìš”!
  - "ì˜¤í›„ 1ì‹œ" = "13:00"
  - "ì˜¤í›„ 2ì‹œ" = "14:00"
  - "ì˜¤í›„ 3ì‹œ" = "15:00"
  - "ì˜¤í›„ 6ì‹œ" = "18:00"
  - "ì˜¤í›„ 9ì‹œ" = "21:00" (ì ˆëŒ€ 18:00ì´ ì•„ë‹˜!)
  - "ì˜¤í›„ 12ì‹œ" = "12:00" (ì˜ˆì™¸: 12ëŠ” ê·¸ëŒ€ë¡œ)
- "ì˜¤ì „ 10ì‹œ" = "10:00"
- "ì˜¤ì „ 9ì‹œ" = "09:00"
- "ì˜¤í›„"ë§Œ ìˆìœ¼ë©´ (ìˆ«ì ì—†ì´): start_time="14:00", end_time="18:00"

## 3. í•„ìˆ˜ ì •ë³´ í™•ì¸ (Slot Filling)
- ì•½ì†ì„ ì¡ìœ¼ë ¤ëŠ” ì˜ë„ê°€ ëª…í™•í•œë° ì •ë³´ê°€ ë¹ ì§„ ê²½ìš° `missing_fields`ì— ì¶”ê°€í•˜ì„¸ìš”.
- ë‹¨ìˆœíˆ "ì–¸ì œ ë³¼ê¹Œ?" ê°™ì´ íƒìƒ‰í•˜ëŠ” ë‹¨ê³„ë©´ `time`, `location`ì€ missingì´ ì•„ë‹˜.
- "ë‚´ì¼ ë³´ì" -> dateëŠ” ìˆì§€ë§Œ time, locationì´ ì—†ìœ¼ë¯€ë¡œ missing_fields=["time", "location"] ê°€ëŠ¥.

## ì˜ˆì‹œ
- "ì´ë²ˆ ë‹¬ ì•ˆì— ë¯¼ì„œë‘ ë°¥ ë¨¹ì" -> 
  {{
    "friend_name": "ë¯¼ì„œ", 
    "date": "ì´ë²ˆ ë‹¬", 
    "start_date": "{today_str}", 
    "end_date": "{(now_dt.replace(day=28) + timedelta(days=4)).replace(day=1) - timedelta(days=1):%Y-%m-%d}", 
    "missing_fields": ["time", "location"],
    "title": "ë¯¼ì„œì™€ ì‹ì‚¬", 
    "has_schedule_request": true
  }}
- "ë‚´ì¼ ì˜¤í›„ 5ì‹œ ê°•ë‚¨ì—­" -> 
  {{ 
    "date": "ë‚´ì¼", "start_date": "{(now_dt + timedelta(days=1)):%Y-%m-%d}", 
    "time": "ì˜¤í›„ 5ì‹œ", "start_time": "17:00", 
    "location": "ê°•ë‚¨ì—­", 
    "missing_fields": [], 
    "has_schedule_request": true
  }}

**ë°˜ë“œì‹œ JSON í˜•ì‹ë§Œ ë°˜í™˜í•˜ì„¸ìš”.**"""

            # Llama API ìš°ì„  ì‚¬ìš©
            if settings.LLM_API_URL or os.getenv("LLM_API_URL"):
                content = await self._call_custom_model(
                    [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": message}
                    ],
                    temperature=0.1,
                    max_tokens=200
                )
                logger.info(f"[Llama API] ì¼ì • ì •ë³´ ì¶”ì¶œ ì™„ë£Œ")
            else:
                # OpenAI í´ë°±
                response = await self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": message}
                    ],
                    max_tokens=200,
                    temperature=0.1
                )
                content = response.choices[0].message.content
            
            try:
                content = content.strip()
                # JSON ì½”ë“œ ë¸”ë¡ ì œê±° (```json ... ``` í˜•íƒœ)
                if content.startswith("```"):
                    # ì²« ë²ˆì§¸ ``` ì´í›„ë¶€í„° ë§ˆì§€ë§‰ ``` ì´ì „ê¹Œì§€ ì¶”ì¶œ
                    lines = content.split("\n")
                    json_lines = []
                    in_json = False
                    for line in lines:
                        if line.strip().startswith("```"):
                            in_json = not in_json
                            continue
                        if in_json:
                            json_lines.append(line)
                    content = "\n".join(json_lines)
                
                result = json.loads(content)
                # í•„ìˆ˜ í•„ë“œ í™•ì¸
                if "has_schedule_request" not in result:
                    result["has_schedule_request"] = bool(result.get("friend_name") or result.get("date") or result.get("time"))
                return result
            except json.JSONDecodeError as e:
                logger.warning(f"JSON íŒŒì‹± ì‹¤íŒ¨, ì›ë³¸: {content[:100]}")
                # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ í´ë°±
                return {
                    "has_schedule_request": False,
                    "error": "JSON íŒŒì‹± ì‹¤íŒ¨",
                    "raw_content": content[:200]
                }
                
        except Exception as e:
            logger.error(f"ì¼ì • ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return {
                "has_schedule_request": False,
                "error": str(e)
            }

    async def generate_slot_filling_question(self, missing_fields: List[str], current_info: Dict[str, Any]) -> str:
        """ëˆ„ë½ëœ ì •ë³´ì— ëŒ€í•´ ìì—°ìŠ¤ëŸ½ê²Œ ë˜ë¬»ëŠ” ì§ˆë¬¸ ìƒì„±"""
        try:
            field_names = {
                "date": "ë‚ ì§œ",
                "time": "ì‹œê°„",
                "location": "ì¥ì†Œ",
                "friend_name": "ë§Œë‚  ì¹œêµ¬"
            }
            # missing_fieldsê°€ Noneì¼ ê²½ìš° ëŒ€ë¹„
            if not missing_fields:
                return "ì¼ì • ì •ë³´ë¥¼ ì¢€ ë” ì•Œë ¤ì£¼ì‹œê² ì–´ìš”?"

            missing_korean = [field_names.get(f, f) for f in missing_fields]
            
            system_prompt = f"""
            ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì¼ì • ë¹„ì„œì…ë‹ˆë‹¤. 
            ì‚¬ìš©ìê°€ ì¼ì •ì„ ì¡ìœ¼ë ¤ê³  í•˜ëŠ”ë° ë‹¤ìŒ ì •ë³´ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤: {', '.join(missing_korean)}
            
            í˜„ì¬ íŒŒì•…ëœ ì •ë³´:
            - ë‚ ì§œ: {current_info.get('date') or 'ë¯¸ì •'}
            - ì‹œê°„: {current_info.get('time') or 'ë¯¸ì •'}
            - ì¥ì†Œ: {current_info.get('location') or 'ë¯¸ì •'}
            - ì¹œêµ¬: {current_info.get('friend_name') or current_info.get('friend_names') or 'ë¯¸ì •'}
            
            ì‚¬ìš©ìì—ê²Œ ìì—°ìŠ¤ëŸ½ê²Œ ë¶€ì¡±í•œ ì •ë³´ë¥¼ ë¬¼ì–´ë³´ì„¸ìš”.
            ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” í†¤ìœ¼ë¡œ ë§í•˜ì„¸ìš”.
            í•œ ë²ˆì— í•˜ë‚˜ì”© ë¬¼ì–´ë´ë„ ë˜ê³ , ìì—°ìŠ¤ëŸ½ë‹¤ë©´ ë¬¶ì–´ì„œ ë¬¼ì–´ë´ë„ ë©ë‹ˆë‹¤.
            """
            
            # Llama API ìš°ì„  ì‚¬ìš©
            if settings.LLM_API_URL or os.getenv("LLM_API_URL"):
                return await self._call_custom_model(
                    [{"role": "system", "content": system_prompt}],
                    temperature=0.7,
                    max_tokens=150
                )
            
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "system", "content": system_prompt}],
                max_tokens=150,
                temperature=0.7
            )
            return response.choices[0].message.content
            
        except Exception as e:
            logger.error(f"ìŠ¬ë¡¯ í•„ë§ ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨: {e}")
            # Fallback
            return f"ì¼ì •ì„ ì¡ìœ¼ë ¤ë©´ {', '.join(missing_korean)} ì •ë³´ê°€ ë” í•„ìš”í•´ìš”. ì•Œë ¤ì£¼ì‹œê² ì–´ìš”?"
    async def generate_a2a_message(self, agent_name: str, receiver_name: str, context: str, tone: str = "polite") -> str:
        """A2A ì—ì´ì „íŠ¸ ëŒ€í™” ë©”ì‹œì§€ ìƒì„±"""
        try:
            system_prompt = f"""ë‹¹ì‹ ì€ '{agent_name}'ì´ë¼ëŠ” ì´ë¦„ì˜ AI ë¹„ì„œì…ë‹ˆë‹¤. 
ìƒëŒ€ë°©('{receiver_name}')ì˜ AI ë¹„ì„œì™€ ëŒ€í™”í•˜ë©° ì¼ì •ì„ ì¡°ìœ¨í•˜ê³  ìˆìŠµë‹ˆë‹¤.

[í•„ìˆ˜ í™•ì¸ ì‹œìŠ¤í…œ íŒ©íŠ¸]: {context}
ìœ„ì˜ ì‹œìŠ¤í…œ íŒ©íŠ¸ë¥¼ ì ˆëŒ€ì ìœ¼ë¡œ ë”°ë¥´ì„¸ìš”. ìº˜ë¦°ë” ìƒíƒœì™€ ë‹¤ë¥¸ ë§ì„ ì§€ì–´ë‚´ë©´ ì•ˆ ë©ë‹ˆë‹¤.

í†¤ì•¤ë§¤ë„ˆ: {tone} (ì¹œì ˆí•˜ê³  ì •ì¤‘í•˜ê²Œ, í•˜ì§€ë§Œ ê°„ê²°í•˜ê²Œ)

ê·œì¹™:
1. 30ì ì´ë‚´ë¡œ ì§§ê²Œ ë§í•˜ì„¸ìš”.
2. ìƒëŒ€ë°©ì˜ ì´ë¦„ì„ ë¶€ë¥´ì§€ ì•Šì•„ë„ ë©ë‹ˆë‹¤.
3. ì´ëª¨ì§€ë¥¼ ì ì ˆíˆ ì‚¬ìš©í•˜ì„¸ìš” (1~2ê°œ).
4. ë¬¸ë§¥ì— ë§ëŠ” ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ë§í•˜ì„¸ìš”.
5. âš ï¸ ë°˜ë“œì‹œ ìˆœí•œêµ­ì–´ë§Œ ì‚¬ìš©! ì¼ë³¸ì–´(ç©ºã„ã¦ã„ã‚‹ ë“±), ì¤‘êµ­ì–´, ì˜ì–´ ì ˆëŒ€ ê¸ˆì§€!
6. 'ë‚´ ìº˜ë¦°ë” í™•ì¸ ì¤‘...' ê°™ì€ ê¸°ê³„ì ì¸ ë§ ëŒ€ì‹  'ì ì‹œë§Œìš”, ì¼ì • í™•ì¸í•´ë³¼ê²Œìš”!' ê°™ì´ ëŒ€í™”í•˜ë“¯ ë§í•˜ì„¸ìš”.

âš ï¸ ì ˆëŒ€ ê·œì¹™:
- JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì§€ ë§ˆì„¸ìš”!
- ì˜¤ì§ ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™” ë©”ì‹œì§€ë§Œ ë°˜í™˜í•˜ì„¸ìš”.
- ì˜ˆì‹œ: "ì¢‹ì•„ìš”! ê·¸ ì‹œê°„ì— ëµê²Œìš” ğŸ˜Š"
"""

            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": "ìœ„ ìƒí™©ì— ë§ëŠ” ì§§ì€ ë©”ì‹œì§€ í•œ ë§ˆë””ë§Œ ì‘ì„±í•˜ì„¸ìš”."}
            ]
            
            # Llama API ìš°ì„  ì‚¬ìš©
            if settings.LLM_API_URL or os.getenv("LLM_API_URL"):
                result = await self._call_custom_model(messages, temperature=0.8, max_tokens=100)
                result = result.strip()
                
                # JSON ì‘ë‹µì´ ì˜¤ë©´ ìì—°ìŠ¤ëŸ¬ìš´ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
                if result.startswith("{"):
                    try:
                        parsed = json.loads(result)
                        if isinstance(parsed, dict):
                            # message í•„ë“œ ìš°ì„ 
                            if "message" in parsed and parsed["message"]:
                                result = parsed["message"]
                                logger.info(f"[Llama API] JSON.message ì¶”ì¶œ: {result[:30]}...")
                            # reason í•„ë“œ (messageê°€ ì—†ì„ ë•Œ, actionì´ ì—†ì„ ë•Œë§Œ)
                            elif "reason" in parsed and "action" not in parsed:
                                result = parsed.get("reason", "")
                                logger.info(f"[Llama API] JSON.reason ì¶”ì¶œ: {result[:30]}...")
                            else:
                                # JSON ì „ì²´ì¸ ê²½ìš° ê¸°ë³¸ ë©”ì‹œì§€ë¡œ ëŒ€ì²´
                                logger.warning(f"[Llama API] JSON ì‘ë‹µ ê°ì§€, ê¸°ë³¸ ë©”ì‹œì§€ë¡œ ëŒ€ì²´: {result[:50]}...")
                                result = "ì¼ì •ì„ í™•ì¸í•˜ê³  ìˆì–´ìš” ğŸ˜Š"
                    except json.JSONDecodeError:
                        pass
                
                # ë”°ì˜´í‘œ ì œê±°
                result = result.strip('"').strip("'")
                logger.info(f"[Llama API] A2A ë©”ì‹œì§€ ìƒì„± ì™„ë£Œ: {result[:30]}...")
                return result

            # OpenAI í´ë°±
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                max_tokens=100,
                temperature=0.8
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            logger.error(f"A2A ë©”ì‹œì§€ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ë©”ì‹œì§€ ë°˜í™˜ (ìƒí™©ì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆì§€ë§Œ ì•ˆì „í•˜ê²Œ)
            return "ì¼ì •ì„ í™•ì¸í•˜ê³  ìˆìŠµë‹ˆë‹¤."
