import json
import logging
from datetime import datetime, timedelta
from typing import Dict, Any, List
from zoneinfo import ZoneInfo

from openai import AsyncOpenAI
import os
import httpx

from config.settings import settings

logger = logging.getLogger(__name__)

class OpenAIService:
    def __init__(self):
        self.client = AsyncOpenAI(api_key=settings.OPENAI_API_KEY)
        self.model = settings.OPENAI_MODEL
    
    async def request_chat_completion(self, messages: List[Dict[str, str]], temperature: float = 0.7, max_tokens: int = 200) -> str:
        """Llama ë˜ëŠ” OpenAI ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì±„íŒ… ì‘ë‹µ ìƒì„± (í†µí•© ë©”ì„œë“œ)"""
        # Llama API ìš°ì„  ì‚¬ìš©
        if settings.LLM_API_URL or os.getenv("LLM_API_URL"):
            return await self._call_custom_model(messages, temperature, max_tokens)
        
        # OpenAI í´ë°±
        response = await self.client.chat.completions.create(
            model=self.model,
            messages=messages,
            max_tokens=max_tokens,
            temperature=temperature
        )
        return response.choices[0].message.content.strip()

    async def _call_custom_model(self, messages: List[Dict[str, str]], temperature: float = 0.7, max_tokens: int = 500) -> str:
        """ì»¤ìŠ¤í…€ LLM (Llama ë“±) í˜¸ì¶œ - ìƒˆ API ìŠ¤í™"""
        url = settings.LLM_API_URL or os.getenv("LLM_API_URL")
        if not url:
            raise ValueError("LLM_API_URL not set")

        # ìƒˆ API ìŠ¤í™: messages ë°°ì—´ ê·¸ëŒ€ë¡œ ì „ì†¡
        payload = {
            "messages": messages,
            "temperature": temperature,
            "max_tokens": max_tokens
        }
        
        logger.info(f"[Llama API] ìš”ì²­ ì „ì†¡: {url}")
        logger.debug(f"[Llama API] Payload: {len(messages)}ê°œ ë©”ì‹œì§€")
        
        async with httpx.AsyncClient(timeout=120.0) as client:
            resp = await client.post(url, json=payload)
            resp.raise_for_status()
            data = resp.json()
            response_text = data.get("response", "")
            logger.info(f"[Llama API] ì‘ë‹µ ìˆ˜ì‹ : {len(response_text)}ì")
            return response_text

    def _get_current_time_info(self) -> str:
        """í˜„ì¬ ì‹œê°„ ì •ë³´ë¥¼ ë¬¸ìì—´ë¡œ ë°˜í™˜"""
        KST = ZoneInfo("Asia/Seoul")
        now = datetime.now(KST)
        
        # ìš”ì¼ì„ í•œê¸€ë¡œ ë³€í™˜
        weekday_map = {
            0: "ì›”ìš”ì¼",
            1: "í™”ìš”ì¼", 
            2: "ìˆ˜ìš”ì¼",
            3: "ëª©ìš”ì¼",
            4: "ê¸ˆìš”ì¼",
            5: "í† ìš”ì¼",
            6: "ì¼ìš”ì¼"
        }
        
        weekday_kr = weekday_map[now.weekday()]
        return now.strftime(f"%Yë…„ %mì›” %dì¼ {weekday_kr} %Hì‹œ %Më¶„ (í•œêµ­ ì‹œê°„)")
    
    async def generate_response(self, user_message: str, conversation_history: List[Dict[str, str]] = None) -> Dict[str, Any]:
        """ChatGPT APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ìƒì„±"""
        try:
            current_time = self._get_current_time_info()
            
            system_prompt = f"""ë‹¹ì‹ ì€ ì¼ì • ë„ìš°ë¯¸ JOYNERì…ë‹ˆë‹¤.
í˜„ì¬ ì‹œê°„: {current_time}

## ì ˆëŒ€ ê·œì¹™ (ë°˜ë“œì‹œ ì§€ì¼œì•¼ í•¨!)
1. ì‚¬ìš©ìê°€ ë§í•˜ì§€ ì•Šì€ ì •ë³´ë¥¼ ë§Œë“¤ì–´ë‚´ì§€ ë§ˆì„¸ìš”!
2. ì¹œêµ¬ ì´ë¦„ì´ ì—†ìœ¼ë©´ ê°œì¸ ì¼ì •ì…ë‹ˆë‹¤
3. ì¥ì†Œ, ì‹œê°„ì€ ì‚¬ìš©ìê°€ ë§í•œ ê²ƒë§Œ ì‚¬ìš©í•˜ì„¸ìš”
4. ê¸°ì¡´ ì¼ì •ì„ ì·¨ì†Œí•˜ê±°ë‚˜ ë³€ê²½í•˜ì§€ ë§ˆì„¸ìš” (ì‚¬ìš©ìê°€ ìš”ì²­í•˜ì§€ ì•Šì•˜ë‹¤ë©´)

## ì‘ë‹µ ë°©ì‹
- ì§§ê³  ì¹œê·¼í•˜ê²Œ ë‹µí•˜ì„¸ìš” (1-2ë¬¸ì¥)
- "ì•„ë‹", "ì•„ë‹ˆ" = ë¶€ì •/ëª¨ë¦„ (ì¸ì‚¬ ì•„ë‹˜!)
- í•„ìš”í•œ ì •ë³´ê°€ ì—†ìœ¼ë©´ ê°„ë‹¨íˆ ë¬¼ì–´ë³´ì„¸ìš”

## ì¼ì • ë“±ë¡ íë¦„
1. ë‚ ì§œ í™•ì¸ (ì—†ìœ¼ë©´ ë¬¼ì–´ë³´ê¸°)
2. ì‹œê°„ í™•ì¸ (ì—†ìœ¼ë©´ ë¬¼ì–´ë³´ê¸°)  
3. í™•ì¸ í›„ ë“±ë¡

ì˜ˆì‹œ:
ì‚¬ìš©ì: "ë‚´ì¼ 3ì‹œì— ì¹˜ê³¼ ì˜ˆì•½ ì¼ì • ì¶”ê°€í•´ì¤˜"
AI: "ë„¤, ë‚´ì¼ ì˜¤í›„ 3ì‹œì— 'ì¹˜ê³¼ ì˜ˆì•½' ì¼ì •ì„ ë“±ë¡í• ê²Œìš”. ëë‚˜ëŠ” ì‹œê°„ë„ ì•Œë ¤ì£¼ì‹¤ë˜ìš”?"
ì‚¬ìš©ì: "ëª°ë¼"
AI: "ì•Œê² ìŠµë‹ˆë‹¤! ë‚´ì¼ ì˜¤í›„ 3ì‹œ 'ì¹˜ê³¼ ì˜ˆì•½' ì¼ì •ìœ¼ë¡œ ë“±ë¡í–ˆì–´ìš” âœ…" """

            messages = [{"role": "system", "content": system_prompt}]
            
            if conversation_history:
                # ìµœê·¼ 10ê°œ ëŒ€í™”ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš© (TPM ì œí•œ ê³ ë ¤í•˜ì—¬ ì¶•ì†Œ)
                recent_history = conversation_history[-10:]
                logger.info(f"[OpenAI] ëŒ€í™” íˆìŠ¤í† ë¦¬ {len(recent_history)}ê°œ ì‚¬ìš©")
                for msg in recent_history:
                    if msg.get("type") == "user":
                        messages.append({"role": "user", "content": msg["message"]})
                        logger.debug(f"[OpenAI] íˆìŠ¤í† ë¦¬ - User: {msg['message'][:50]}...")
                    elif msg.get("type") == "assistant":
                        messages.append({"role": "assistant", "content": msg["message"]})
                        logger.debug(f"[OpenAI] íˆìŠ¤í† ë¦¬ - AI: {msg['message'][:50]}...")
            
            messages.append({"role": "user", "content": user_message})
            logger.info(f"[OpenAI] í˜„ì¬ ë©”ì‹œì§€: {user_message}")
            
            # Llama API ìš°ì„  ì‚¬ìš©
            if settings.LLM_API_URL or os.getenv("LLM_API_URL"):
                ai_response = await self._call_custom_model(messages, temperature=0.5, max_tokens=300)
                logger.info(f"[Llama API] ì›ë³¸ ì‘ë‹µ: {ai_response[:100]}...")
                
                # JSON ì‘ë‹µì¸ ê²½ìš° message í•„ë“œ ì¶”ì¶œ
                if ai_response.strip().startswith("{"):
                    try:
                        parsed = json.loads(ai_response)
                        if isinstance(parsed, dict) and "message" in parsed:
                            ai_response = parsed["message"]
                            logger.info(f"[Llama API] JSONì—ì„œ message ì¶”ì¶œ: {ai_response[:50]}...")
                    except json.JSONDecodeError:
                        pass  # JSON ì•„ë‹ˆë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                
                return {
                    "status": "success",
                    "message": ai_response,
                    "usage": {}
                }

            response = await self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                max_tokens=500,
                temperature=0.7
            )
            
            ai_response = response.choices[0].message.content
            
            logger.info(f"OpenAI API ì‘ë‹µ ìƒì„± ì™„ë£Œ: {len(ai_response)}ì")
            
            return {
                "status": "success",
                "message": ai_response,
                "usage": {
                    "prompt_tokens": response.usage.prompt_tokens,
                    "completion_tokens": response.usage.completion_tokens,
                    "total_tokens": response.usage.total_tokens
                }
            }
            
        except Exception as e:
            error_msg = str(e)
            logger.error(f"OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {error_msg}")
            
            user_msg = "ì£„ì†¡í•´ìš”, ì§€ê¸ˆ ì ì‹œ ìƒê°ì´ ì•ˆ ë‚˜ë„¤ìš”. ì ì‹œ í›„ ë‹¤ì‹œ ë§í•´ì£¼ì‹œê² ì–´ìš”?"
            if "insufficient_quota" in error_msg:
                user_msg = "API ì‚¬ìš©ëŸ‰ í•œë„ê°€ ì´ˆê³¼ë˜ì—ˆì–´ìš”. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•´ì£¼ì„¸ìš”."
            elif "rate limit" in error_msg.lower():
                user_msg = "ì§€ê¸ˆ ë„ˆë¬´ ë§ì€ ëŒ€í™”ê°€ ì˜¤ê³  ê°€ê³  ìˆì–´ìš”. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”!"
                
            return {
                "status": "error",
                "message": user_msg,
                "error": error_msg
            }
    
    async def extract_schedule_info(self, message: str) -> Dict[str, Any]:
        """ë©”ì‹œì§€ì—ì„œ ì¼ì • ê´€ë ¨ ì •ë³´ ì¶”ì¶œ"""
        try:
            current_time = self._get_current_time_info()
            
            # í˜„ì¬ ì‹œê°„ ìƒì„¸ ì •ë³´ (YYYY-MM-DD í˜•ì‹ í¬í•¨)
            now_dt = datetime.now(ZoneInfo("Asia/Seoul"))
            today_str = now_dt.strftime("%Y-%m-%d")
            
            system_prompt = f"""ë‹¤ìŒ ë©”ì‹œì§€ì—ì„œ ì¼ì • ê´€ë ¨ ì •ë³´ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.
í˜„ì¬ ì‹œê°: {current_time}
ì˜¤ëŠ˜ ë‚ ì§œ(ê¸°ì¤€): {today_str}

**ì¤‘ìš”: ë°˜ë“œì‹œ ìœ íš¨í•œ JSONë§Œ ë°˜í™˜í•˜ì„¸ìš”.**

JSON ë°˜í™˜ í˜•ì‹:
{{
    "friend_name": "ì¹œêµ¬ ì´ë¦„ (âš ï¸ ì–¸ê¸‰ ì—†ìœ¼ë©´ ë°˜ë“œì‹œ null!)",
    "friend_names": ["ì¹œêµ¬1", "ì¹œêµ¬2"],
    "date": "í…ìŠ¤íŠ¸ ë‚ ì§œ (ì˜ˆ: ì´ë²ˆì£¼ ê¸ˆìš”ì¼)",
    "start_date": "YYYY-MM-DD (ë²”ìœ„ ì‹œì‘)",
    "end_date": "YYYY-MM-DD (ë²”ìœ„ ì¢…ë£Œ)",
    "time": "ì‹œê°„ í…ìŠ¤íŠ¸ (ì˜ˆ: ì €ë…)",
    "start_time": "HH:MM (24ì‹œê°„ì œ)",
    "end_time": "HH:MM (24ì‹œê°„ì œ)",
    "activity": "í™œë™ ë‚´ìš©",
    "title": "ì¼ì • ì œëª©",
    "location": "ì¥ì†Œ",
    "has_schedule_request": true/false,
    "missing_fields": ["date", "time", "location"] (ëˆ„ë½ëœ í•„ìˆ˜ ì •ë³´ ë¦¬ìŠ¤íŠ¸)
}}

## âš ï¸ 0. ê°€ì¥ ì¤‘ìš”í•œ ê·œì¹™: ì •ë³´ë¥¼ ë§Œë“¤ì–´ë‚´ì§€ ë§ˆì„¸ìš”!
- **ì‚¬ìš©ìê°€ ì¹œêµ¬ ì´ë¦„ì„ ëª…ì‹œì ìœ¼ë¡œ ë§í•˜ì§€ ì•Šì•˜ë‹¤ë©´ friend_nameì€ ë°˜ë“œì‹œ nullì´ì–´ì•¼ í•©ë‹ˆë‹¤!**
- "ë¯¼ì„œ", "ê·œë¯¼", "ì§€í˜œ" ë“± ì§ì ‘ ì–¸ê¸‰ë˜ì§€ ì•Šì€ ì´ë¦„ì„ ì§€ì–´ë‚´ë©´ ì•ˆ ë©ë‹ˆë‹¤.
- ì˜ˆì‹œ: "ë‚´ì¼ í•œì‹œì— í‹°ì¼“íŒ… ì˜ˆì•½í•˜ê¸° ì¼ì • ë“±ë¡í•´ì¤˜" â†’ friend_name: null (ì¹œêµ¬ ì–¸ê¸‰ ì—†ìŒ!)
- ì˜ˆì‹œ: "ë¯¼ì„œë‘ ë‚´ì¼ ë°¥ ë¨¹ì" â†’ friend_name: "ë¯¼ì„œ" (ì¹œêµ¬ ì–¸ê¸‰ë¨)

## 1. ë‚ ì§œ ë²”ìœ„ ë³€í™˜ ê·œì¹™ (ì˜¤ëŠ˜: {today_str} ê¸°ì¤€)
- "ì´ë²ˆ ë‹¬": ì˜¤ëŠ˜ë¶€í„° ì´ë²ˆ ë‹¬ ë§ì¼ê¹Œì§€ (start_date ~ end_date)
- "ë‹¤ìŒ ì£¼": ë‹¤ìŒ ì£¼ ì›”ìš”ì¼ ~ ì¼ìš”ì¼
- "ì£¼ë§": ì´ë²ˆ ì£¼ í† ìš”ì¼ ~ ì¼ìš”ì¼ (ì´ë¯¸ ì§€ë‚¬ìœ¼ë©´ ë‹¤ìŒ ì£¼ ì£¼ë§)
- "í‰ì¼": ì›”~ê¸ˆ
- "ì˜¤ëŠ˜": ì˜¤ëŠ˜ ë‚ ì§œ
- "ë‚´ì¼": ì˜¤ëŠ˜ + 1ì¼

## 2. ì‹œê°„ ë³€í™˜ ê·œì¹™ (ë§¤ìš° ì¤‘ìš”!)
- **ë¶„ ë‹¨ìœ„ë„ ì •í™•íˆ ì¶”ì¶œí•˜ì„¸ìš”!**
  - "5ì‹œë°˜" = "17:30" (ë°˜=30ë¶„)
  - "5ì‹œ 30ë¶„" = "17:30"
  - "5:30" = "17:30"
  - "5ì‹œ 15ë¶„" = "17:15"
  - "3ì‹œ 45ë¶„" = "15:45"
- "ì•„ì¹¨": start_time="09:00", end_time="11:00"
- "ì ì‹¬": start_time="12:00", end_time="14:00"
- "ì €ë…": start_time="18:00", end_time="22:00"
- **"ì˜¤í›„" + ìˆ«ì**: ë°˜ë“œì‹œ 12ë¥¼ ë”í•˜ì„¸ìš”!
  - "ì˜¤í›„ 1ì‹œ" = "13:00"
  - "ì˜¤í›„ 2ì‹œ" = "14:00"
  - "ì˜¤í›„ 3ì‹œ" = "15:00"
  - "ì˜¤í›„ 3ì‹œ 30ë¶„" = "15:30"
  - "ì˜¤í›„ 6ì‹œ" = "18:00"
  - "ì˜¤í›„ 9ì‹œ" = "21:00" (ì ˆëŒ€ 18:00ì´ ì•„ë‹˜!)
  - "ì˜¤í›„ 12ì‹œ" = "12:00" (ì˜ˆì™¸: 12ëŠ” ê·¸ëŒ€ë¡œ)
- "ì˜¤ì „ 10ì‹œ" = "10:00"
- "ì˜¤ì „ 9ì‹œ" = "09:00"
- "ì˜¤ì „ 10ì‹œë°˜" = "10:30"
- "ì˜¤í›„"ë§Œ ìˆìœ¼ë©´ (ìˆ«ì ì—†ì´): start_time="14:00", end_time="18:00"

## 3. í•„ìˆ˜ ì •ë³´ í™•ì¸ (Slot Filling)
- ì•½ì†ì„ ì¡ìœ¼ë ¤ëŠ” ì˜ë„ê°€ ëª…í™•í•œë° ì •ë³´ê°€ ë¹ ì§„ ê²½ìš° `missing_fields`ì— ì¶”ê°€í•˜ì„¸ìš”.
- ë‹¨ìˆœíˆ "ì–¸ì œ ë³¼ê¹Œ?" ê°™ì´ íƒìƒ‰í•˜ëŠ” ë‹¨ê³„ë©´ `time`, `location`ì€ missingì´ ì•„ë‹˜.
- "ë‚´ì¼ ë³´ì" -> dateëŠ” ìˆì§€ë§Œ time, locationì´ ì—†ìœ¼ë¯€ë¡œ missing_fields=["time", "location"] ê°€ëŠ¥.

## ì˜ˆì‹œ
- "ì´ë²ˆ ë‹¬ ì•ˆì— ë¯¼ì„œë‘ ë°¥ ë¨¹ì" -> 
  {{
    "friend_name": "ë¯¼ì„œ", 
    "date": "ì´ë²ˆ ë‹¬", 
    "start_date": "{today_str}", 
    "end_date": "{(now_dt.replace(day=28) + timedelta(days=4)).replace(day=1) - timedelta(days=1):%Y-%m-%d}", 
    "missing_fields": ["time", "location"],
    "title": "ë¯¼ì„œì™€ ì‹ì‚¬", 
    "has_schedule_request": true
  }}
- "ë‚´ì¼ ì˜¤í›„ 5ì‹œ ê°•ë‚¨ì—­" -> 
  {{ 
    "friend_name": null,
    "date": "ë‚´ì¼", "start_date": "{(now_dt + timedelta(days=1)):%Y-%m-%d}", 
    "time": "ì˜¤í›„ 5ì‹œ", "start_time": "17:00", 
    "location": "ê°•ë‚¨ì—­", 
    "missing_fields": [], 
    "has_schedule_request": true
  }}
- "ë‚´ì¼ í•œì‹œì— í‹°ì¼“íŒ… ì˜ˆì•½í•˜ê¸° ì¼ì • ë“±ë¡í•´ì¤˜" ->
  {{
    "friend_name": null,
    "date": "ë‚´ì¼", "start_date": "{(now_dt + timedelta(days=1)):%Y-%m-%d}",
    "time": "í•œì‹œ", "start_time": "13:00",
    "title": "í‹°ì¼“íŒ… ì˜ˆì•½í•˜ê¸°",
    "has_schedule_request": true
  }}

**ë°˜ë“œì‹œ JSON í˜•ì‹ë§Œ ë°˜í™˜í•˜ì„¸ìš”.**"""

            # Llama API ìš°ì„  ì‚¬ìš©
            if settings.LLM_API_URL or os.getenv("LLM_API_URL"):
                content = await self._call_custom_model(
                    [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": message}
                    ],
                    temperature=0.1,
                    max_tokens=200
                )
                logger.info(f"[Llama API] ì¼ì • ì •ë³´ ì¶”ì¶œ ì™„ë£Œ")
            else:
                # OpenAI í´ë°±
                response = await self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": message}
                    ],
                    max_tokens=200,
                    temperature=0.1
                )
                content = response.choices[0].message.content
            
            try:
                content = content.strip()
                # JSON ì½”ë“œ ë¸”ë¡ ì œê±° (```json ... ``` í˜•íƒœ)
                if content.startswith("```"):
                    # ì²« ë²ˆì§¸ ``` ì´í›„ë¶€í„° ë§ˆì§€ë§‰ ``` ì´ì „ê¹Œì§€ ì¶”ì¶œ
                    lines = content.split("\n")
                    json_lines = []
                    in_json = False
                    for line in lines:
                        if line.strip().startswith("```"):
                            in_json = not in_json
                            continue
                        if in_json:
                            json_lines.append(line)
                    content = "\n".join(json_lines)
                
                result = json.loads(content)
                # í•„ìˆ˜ í•„ë“œ í™•ì¸
                if "has_schedule_request" not in result:
                    result["has_schedule_request"] = bool(result.get("friend_name") or result.get("date") or result.get("time"))
                return result
            except json.JSONDecodeError as e:
                logger.warning(f"JSON íŒŒì‹± ì‹¤íŒ¨, ì›ë³¸: {content[:100]}")
                # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ í´ë°±
                return {
                    "has_schedule_request": False,
                    "error": "JSON íŒŒì‹± ì‹¤íŒ¨",
                    "raw_content": content[:200]
                }
                
        except Exception as e:
            logger.error(f"ì¼ì • ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return {
                "has_schedule_request": False,
                "error": str(e)
            }

    async def generate_slot_filling_question(self, missing_fields: List[str], current_info: Dict[str, Any]) -> str:
        """ëˆ„ë½ëœ ì •ë³´ì— ëŒ€í•´ ìì—°ìŠ¤ëŸ½ê²Œ ë˜ë¬»ëŠ” ì§ˆë¬¸ ìƒì„±"""
        try:
            field_names = {
                "date": "ë‚ ì§œ",
                "time": "ì‹œê°„",
                "location": "ì¥ì†Œ",
                "friend_name": "ë§Œë‚  ì¹œêµ¬"
            }
            # missing_fieldsê°€ Noneì¼ ê²½ìš° ëŒ€ë¹„
            if not missing_fields:
                return "ì¼ì • ì •ë³´ë¥¼ ì¢€ ë” ì•Œë ¤ì£¼ì‹œê² ì–´ìš”?"

            missing_korean = [field_names.get(f, f) for f in missing_fields]
            
            system_prompt = f"""
            ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì¼ì • ë¹„ì„œì…ë‹ˆë‹¤. 
            ì‚¬ìš©ìê°€ ì¼ì •ì„ ì¡ìœ¼ë ¤ê³  í•˜ëŠ”ë° ë‹¤ìŒ ì •ë³´ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤: {', '.join(missing_korean)}
            
            í˜„ì¬ íŒŒì•…ëœ ì •ë³´:
            - ë‚ ì§œ: {current_info.get('date') or 'ë¯¸ì •'}
            - ì‹œê°„: {current_info.get('time') or 'ë¯¸ì •'}
            - ì¥ì†Œ: {current_info.get('location') or 'ë¯¸ì •'}
            - ì¹œêµ¬: {current_info.get('friend_name') or current_info.get('friend_names') or 'ë¯¸ì •'}
            
            ì‚¬ìš©ìì—ê²Œ ìì—°ìŠ¤ëŸ½ê²Œ ë¶€ì¡±í•œ ì •ë³´ë¥¼ ë¬¼ì–´ë³´ì„¸ìš”.
            ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” í†¤ìœ¼ë¡œ ë§í•˜ì„¸ìš”.
            í•œ ë²ˆì— í•˜ë‚˜ì”© ë¬¼ì–´ë´ë„ ë˜ê³ , ìì—°ìŠ¤ëŸ½ë‹¤ë©´ ë¬¶ì–´ì„œ ë¬¼ì–´ë´ë„ ë©ë‹ˆë‹¤.
            """
            
            # Llama API ìš°ì„  ì‚¬ìš©
            if settings.LLM_API_URL or os.getenv("LLM_API_URL"):
                return await self._call_custom_model(
                    [{"role": "system", "content": system_prompt}],
                    temperature=0.7,
                    max_tokens=150
                )
            
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "system", "content": system_prompt}],
                max_tokens=150,
                temperature=0.7
            )
            return response.choices[0].message.content
            
        except Exception as e:
            logger.error(f"ìŠ¬ë¡¯ í•„ë§ ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨: {e}")
            # Fallback
            return f"ì¼ì •ì„ ì¡ìœ¼ë ¤ë©´ {', '.join(missing_korean)} ì •ë³´ê°€ ë” í•„ìš”í•´ìš”. ì•Œë ¤ì£¼ì‹œê² ì–´ìš”?"
    async def generate_a2a_message(self, agent_name: str, receiver_name: str, context: str, tone: str = "polite") -> str:
        """A2A ì—ì´ì „íŠ¸ ëŒ€í™” ë©”ì‹œì§€ ìƒì„±"""
        try:
            system_prompt = f"""ë‹¹ì‹ ì€ '{agent_name}'ì´ë¼ëŠ” ì´ë¦„ì˜ AI ë¹„ì„œì…ë‹ˆë‹¤. 
ìƒëŒ€ë°©('{receiver_name}')ì˜ AI ë¹„ì„œì™€ ëŒ€í™”í•˜ë©° ì¼ì •ì„ ì¡°ìœ¨í•˜ê³  ìˆìŠµë‹ˆë‹¤.

[í•„ìˆ˜ í™•ì¸ ì‹œìŠ¤í…œ íŒ©íŠ¸]: {context}
ìœ„ì˜ ì‹œìŠ¤í…œ íŒ©íŠ¸ë¥¼ ì ˆëŒ€ì ìœ¼ë¡œ ë”°ë¥´ì„¸ìš”. ìº˜ë¦°ë” ìƒíƒœì™€ ë‹¤ë¥¸ ë§ì„ ì§€ì–´ë‚´ë©´ ì•ˆ ë©ë‹ˆë‹¤.

í†¤ì•¤ë§¤ë„ˆ: {tone} (ì¹œì ˆí•˜ê³  ì •ì¤‘í•˜ê²Œ, í•˜ì§€ë§Œ ê°„ê²°í•˜ê²Œ)

ê·œì¹™:
1. 30ì ì´ë‚´ë¡œ ì§§ê²Œ ë§í•˜ì„¸ìš”.
2. ìƒëŒ€ë°©ì˜ ì´ë¦„ì„ ë¶€ë¥´ì§€ ì•Šì•„ë„ ë©ë‹ˆë‹¤.
3. ì´ëª¨ì§€ë¥¼ ì ì ˆíˆ ì‚¬ìš©í•˜ì„¸ìš” (1~2ê°œ).
4. ë¬¸ë§¥ì— ë§ëŠ” ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ë§í•˜ì„¸ìš”.
5. âš ï¸ ë°˜ë“œì‹œ ìˆœí•œêµ­ì–´ë§Œ ì‚¬ìš©! ì¼ë³¸ì–´(ç©ºã„ã¦ã„ã‚‹ ë“±), ì¤‘êµ­ì–´, ì˜ì–´ ì ˆëŒ€ ê¸ˆì§€!
6. 'ë‚´ ìº˜ë¦°ë” í™•ì¸ ì¤‘...' ê°™ì€ ê¸°ê³„ì ì¸ ë§ ëŒ€ì‹  'ì ì‹œë§Œìš”, ì¼ì • í™•ì¸í•´ë³¼ê²Œìš”!' ê°™ì´ ëŒ€í™”í•˜ë“¯ ë§í•˜ì„¸ìš”.

âš ï¸ ì ˆëŒ€ ê·œì¹™:
- JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì§€ ë§ˆì„¸ìš”!
- ì˜¤ì§ ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™” ë©”ì‹œì§€ë§Œ ë°˜í™˜í•˜ì„¸ìš”.
- ì˜ˆì‹œ: "ì¢‹ì•„ìš”! ê·¸ ì‹œê°„ì— ëµê²Œìš” ğŸ˜Š"
"""

            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": "ìœ„ ìƒí™©ì— ë§ëŠ” ì§§ì€ ë©”ì‹œì§€ í•œ ë§ˆë””ë§Œ ì‘ì„±í•˜ì„¸ìš”."}
            ]
            
            # Llama API ìš°ì„  ì‚¬ìš©
            if settings.LLM_API_URL or os.getenv("LLM_API_URL"):
                result = await self._call_custom_model(messages, temperature=0.8, max_tokens=100)
                result = result.strip()
                
                # JSON ì‘ë‹µì´ ì˜¤ë©´ ìì—°ìŠ¤ëŸ¬ìš´ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
                if result.startswith("{"):
                    try:
                        parsed = json.loads(result)
                        if isinstance(parsed, dict):
                            # message í•„ë“œ ìš°ì„ 
                            if "message" in parsed and parsed["message"]:
                                result = parsed["message"]
                                logger.info(f"[Llama API] JSON.message ì¶”ì¶œ: {result[:30]}...")
                            # reason í•„ë“œ (messageê°€ ì—†ì„ ë•Œ, actionì´ ì—†ì„ ë•Œë§Œ)
                            elif "reason" in parsed and "action" not in parsed:
                                result = parsed.get("reason", "")
                                logger.info(f"[Llama API] JSON.reason ì¶”ì¶œ: {result[:30]}...")
                            else:
                                # JSON ì „ì²´ì¸ ê²½ìš° ê¸°ë³¸ ë©”ì‹œì§€ë¡œ ëŒ€ì²´
                                logger.warning(f"[Llama API] JSON ì‘ë‹µ ê°ì§€, ê¸°ë³¸ ë©”ì‹œì§€ë¡œ ëŒ€ì²´: {result[:50]}...")
                                result = "ì¼ì •ì„ í™•ì¸í•˜ê³  ìˆì–´ìš” ğŸ˜Š"
                    except json.JSONDecodeError:
                        pass
                
                # ë”°ì˜´í‘œ ì œê±°
                result = result.strip('"').strip("'")
                logger.info(f"[Llama API] A2A ë©”ì‹œì§€ ìƒì„± ì™„ë£Œ: {result[:30]}...")
                return result

            # OpenAI í´ë°±
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                max_tokens=100,
                temperature=0.8
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            logger.error(f"A2A ë©”ì‹œì§€ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ë©”ì‹œì§€ ë°˜í™˜ (ìƒí™©ì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆì§€ë§Œ ì•ˆì „í•˜ê²Œ)
            return "ì¼ì •ì„ í™•ì¸í•˜ê³  ìˆìŠµë‹ˆë‹¤."
